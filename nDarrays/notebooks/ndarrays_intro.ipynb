{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "data_dir = '/data/tutorials/nDarrays/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(__NOTE__: here and elsewhere, replace `<data_dir>` with the full path to your own data directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional Arrays - Intro Tutorial\n",
    "======================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: datasets for the xarray tutorial\n",
    "\n",
    "---\n",
    "Questions:\n",
    "- \"What sample datasets will we use in this tutorial?\"\n",
    "objectives:\n",
    "- \"Learn about how we acquired climate reanalysis datasets\"\n",
    "keypoints:\n",
    "- \"refer to this page for access to the tutorial data\"\n",
    "---\n",
    "\n",
    "### Reanalysis datasets:\n",
    "\n",
    "We will be exploring the xarray architecture using some sample climate data from the European Centre for Medium-Range Weather Forecasts ([ECMWF](http://www.ecmwf.int/)). We will use their ERA-Intrim climate reanalysis project. You can download the data in netCDF format [here](http://apps.ecmwf.int/datasets/data/interim-full-daily/levtype=sfc/). As is the case for many climate products, the process involves downloading large netCDF files to a local machine.\n",
    "\n",
    "If you visit the ECMWF page you will find that you can download a large number of different climate fields. Here we have prepared tutorial examples around 4 variables. Note that we provide a full resolution (global) version as well as a subsetted version (Alaska). Choose the Alaska data if you are limited for space or processing power on your local computer. The tutorials exercises will work for either set of data.\n",
    "\n",
    "| Dataset | Description | Size (MB) |\n",
    "|:-------------:|:-------------:|:------------:|\n",
    "| airtemp_global.nc | 2 meter air temperature | 3.0 |\n",
    "| uwind_global.nc | wind blowing to the east | 3.0 |\n",
    "| vwind_global.nc | wind blowing to the north | 3.0 |\n",
    "| SST_global.nc | sea surface temperature | 3.0 |\n",
    "| | | |\n",
    "| airtemp_AK.nc | 2 meter air temperature | 0.1 |\n",
    "| uwind_AK.nc | wind blowing to the east | 0.1 |\n",
    "| vwind_AK.nc | wind blowing to the north | 0.1 |\n",
    "| SST_AK.nc | sea surface temperature | 0.1 |\n",
    "\n",
    "Note 1: when loading the data into xarray, we will use \"engine = scipy\" for all of the global datasets. You do not need to specify an engine for the AK datasets.\n",
    "\n",
    "Note 2: many of our examples follow from and expand on xarray developer Stephan Hoyer's [blog post](https://www.continuum.io/content/xray-dask-out-core-labeled-arrays-python).\n",
    "\n",
    "## Data Access:\n",
    "\n",
    "* Alaska subsets are pre-loaded into the docker containers\n",
    "* via ftp: host = ftp.cloudmaven.org (user/password to be provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "def open_tutorial_data(kind='AK', **kwargs):\n",
    "    if kind.upper() == 'AK':\n",
    "        ds = xr.open_mfdataset(os.path.join(data_dir, '*_AK.nc'), **kwargs)\n",
    "    elif kind.upper() == 'GLOBAL':\n",
    "        ds = xr.open_mfdataset(os.path.join(data_dir, '*_global.nc'), **kwargs)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = open_tutorial_data('AK')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to multidimensional arrays\n",
    "\n",
    "---\n",
    "Questions:\n",
    "- \"When do we need to use multidimensional arrays?\"\n",
    "- \"What are current challenges is manipulating these datasets?\"\n",
    "objectives:\n",
    "- explore how most people currently handle these types of datasets\n",
    "- discuss how current methods are limiting the science that can be accomplished\n",
    "keypoints:\n",
    "- unlabelled, N-dimensional arrays of numbers (e.g. NumPy's ndarray) are the most widely used data structure in scientific computing\n",
    "- these arrays lack meaningful metadata, so users must track indices in an arbitrary fashion\n",
    "- in-memory operations, needed to process and visualize large arrays, are reaching limits as datasets grow in size\n",
    "---\n",
    "### Overview:\n",
    "\n",
    "Unlabelled, N-dimensional arrays of numbers, such as NumPy's ndarray, are the most widely used data structure in scientific computing. Geoscientists have a particular need for structuring their data as arrays. For example, we commonly work with sets of climate variables (e.g. temperature and precipitation) that vary in space and time and are represented on a regularly-spaced grid. Often we need to subset a large global grid to look at data for a particular region, or select a specific time slice. Then we might want to apply statistical functions to these subsetted groups to generate summary information.\n",
    "\n",
    "<br>\n",
    "<img src=\"http://xarray.pydata.org/en/stable/_images/dataset-diagram.png\" width = \"800\" border = \"10\">\n",
    "<br>\n",
    "\n",
    "> ## Isn't this the same as raster processing?\n",
    "> The tools in this tutorial have some similarity to raster image processing tools.\n",
    "> Both require computational engines that can manipulate large stacks of data formatted as arrays.\n",
    "> Here we focus on tools that are optimized to handle data that have many variables spanning dimensions\n",
    "> of time and space. See the raster tutorials for tools that are optimized for image processing of remote sensing datasets.\n",
    "\n",
    "\n",
    "### Conventional Approach: Working with Unlabelled Arrays\n",
    "\n",
    "Multidimensional array data are often stored in user-defined binary formats, and distributed with custom Fortran\n",
    "or C++ libraries used to read and process the data. Users are responsible for setting up their own file structures and custom codes to handle these files. Subsetting the data involves reading everything into an in-memory array, and then using a series of nested loops with conditional statements to look for a specific range of index values associated with the temporal or spatial slice needed. Also, clever use of matrix algebra is often used to summarize data across spatial and temporal dimensions.\n",
    "\n",
    "### Challenges:\n",
    "\n",
    "The biggest challenge in working with N-dimensional arrays in this fashion is the fact that the data are almost disassociated from their metadata. Users are left with the task of tracking the meaning behind array indices using domain-specific software, often leading to inefficiencies and errors. Common pitfalls often occur in in the form of questions like \"is the time axis of my array in the first or third index position?\", or \"does my array of timestamps still align with my data after resampling?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sst = np.random.random(size=(31, 10, 10))\n",
    "\n",
    "print(sst[:, ::3, ::3])  # some fancy indexing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "<br>\n",
    "<img src=\"http://pandas.pydata.org/_static/pandas_logo.png\" width = \"800\" border = \"10\">\n",
    "<br>\n",
    "\n",
    "[Pandas](http://pandas.pydata.org/) is a tabular data library that provides fast data structures and tools for working with a variety of 1 and 2 dimensional data formats. `Pandas` is well suited for many different kinds of data:\n",
    "\n",
    "- Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n",
    "- Ordered and unordered (not necessarily fixed-frequency) time series data.\n",
    "- Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels\n",
    "- Any other form of observational / statistical data sets. The data actually need not be labeled at all to be placed into a pandas data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://climate.nasa.gov/system/internal_resources/details/original/647_Global_Temperature_Data_File.txt',\n",
    "                 sep=r\"\\s*\", names=['year', '1yr', '5yr'], index_col='year')\n",
    "\n",
    "df.loc[1984: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network Common Data Format\n",
    "\n",
    "The network Common Data Form, or [netCDF](http://www.unidata.ucar.edu/software/netcdf/docs/), was created in the early 1990s, and set out to solve some of the challenges in working with N-dimensional arrays. Netcdf is a collection of self-describing, machine-independent binary data formats and software tools that facilitate the creation, access and sharing of scientific data stored in N-dimensional arrays, along with metadata describing the contents of each array. Netcdf was built by the climate science community at a time when regional climate models were beginning to produce larger and larger output files. Another  format, [HDF5](https://www.hdfgroup.org/), has been used for many applications including distribution of remote sensing datasets. It turns out these two formats are now merging, such that the latest version netCDF-4 is the HDF5 format but with some restrictions.\n",
    "\n",
    "One benefit of Common Data Formats is that they are structured in ways that enable rapid subsetting and analysis using simple command line tools. For example, the climate community has developed their own [netCDF toolkits](http://www.unidata.ucar.edu/software/netcdf/software.html) that accomplish tasks like subsetting and grouping. Similar tools exist for [HDF5](https://support.hdfgroup.org/HDF5/Tutor/HDF5Intro.pdf). Therefore many researchers utilize these tools exclusively in their analysis.\n",
    "\n",
    "### NetCDF in practice\n",
    "\n",
    "NetCDF has been widely adopted as a standard format for distributing N-dimensional arrays. Although many geoscience communities rely entirely on existing NetCDF software tools for processing and visualizing their data, others simply use NetCDF as a convenient format for serializing their arrays. In many applications, existing NetCDF tools do not provide the flexibility needed for a specific research question, and users end up reading arrays into memory. They then perform statistical and subsetting operations using conventional coding methods (e.g. looping over array indices) described above.\n",
    "\n",
    "### Handling large arrays\n",
    "\n",
    "The NetCDF format has no limit on file sizes. However, any analysis tools that read data from a NetCDF array into memory for some computational operation will be limited by that particular machine's available memory. As many multidimensional datasets grown in size, for example due to increases in model resolution and remote sensing capabilities, we are becoming increasingly limited in our ability to handle these large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xarray architecture\n",
    "\n",
    "---\n",
    "Questions:\n",
    "- \"What functionality does the xarray library offer?\"\n",
    "- \"What are the benefits and limitations of this library?\"\n",
    "- \"What is the fundamental architecture of xarray data objects?\"\n",
    "objectives:\n",
    "- \"learning the xarray data model\"\n",
    "- \"selection and subsetting of array datasets using labeled indexing\"\n",
    "keypoints:\n",
    "- \"xarray is build on the netCDF data model\"\n",
    "- \"xarray has two main data structures: DataArray and Dataset\"\n",
    "- \"DataArrays store the multi-dimensional arrays\"\n",
    "- \"Datasets are the multi-dimensional equivalent of a Pandas dataframe\"\n",
    "---\n",
    "\n",
    "### What is xarray?\n",
    "\n",
    "* originally developed by employees (Stephan Hoyer, Alex Kleeman and Eugene Brevdo) at [The Climate Corporation](https://climate.com/)\n",
    "* xaray extends some of the core functionality of the Pandas library:\n",
    "    * operations over _named_ dimensions\n",
    "    * selection by label instead of integer location\n",
    "    * powerful _groupby_ functionality\n",
    "    * database-like joins\n",
    "\n",
    "### When to use xarray:\n",
    "\n",
    "* if your data are multidimensional (e.g. climate data: x, y, z, time)\n",
    "* if your data are structured on a regular grid\n",
    "* if you can represent your data in netCDF format\n",
    "\n",
    "### Basic xarray data structures:\n",
    "* NetCDF forms the basis of the xarray data structure\n",
    "* two main data structures are the `DataArray` and the `Dataset`\n",
    "\n",
    "#### `DataArray`\n",
    "* the `DataArray` is xarray's implementation of a labeled, multi-dimensional array\n",
    "* the `DataArray` has these key properties:\n",
    "  * `data`: N-dimensional array (NumPy or dask) holding the array's values,\n",
    "  * `dims`: dimension names for each axis,\n",
    "  * `coords`: dictionary-like container of arrays that label each point, and\n",
    "  * `attrs`: ordered dictionary holding metadata\n",
    "\n",
    "<br><br><br>\n",
    "<img src=\"http://xray.readthedocs.org/en/stable/_images/dataset-diagram.png\" width = \"800\" border = \"10\">\n",
    "<br><br><br>\n",
    "* dimensions (x, y, time); variables (temp, precip); coords (lat, long); attributes\n",
    "\n",
    "\n",
    "### `Dataset`\n",
    "* xarray's multi-dimensional equivalent of a Pandas `DataFrame`\n",
    "* dict-like container of DataArray objects with aligned dimensions\n",
    "* Datasets have these key properties:\n",
    "  * `dims`: dictionary mapping from dimension names to the fixed length of each dimension,\n",
    "  * `data_vars`: dict-like container of `DataArrays` corresponding to data variables,\n",
    "  * `coords`: dictionary-like container of `DataArrays` intended to label points used in data_vars\n",
    "  * `attrs`: ordered dictionary holding metadata\n",
    "\n",
    "### begin by importing the xarray library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the dataset\n",
    "\n",
    "First we [open](http://xarray.pydata.org/en/stable/generated/xarray.open_dataset.html) the data and load it into a `Dataset`. (Note: the choice of engine depends on the format of the netCDF file. See our [dataset description](../00-datasets) lesson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(os.path.join(data_dir, 'airtemp_global.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice this seemed to go very fast. That is because this step does not actually ask Python to read the data into memory. Rather, Python is just scanning the contents of the file. This is called _lazy loading_.\n",
    "\n",
    "## `Dataset` Properties\n",
    "\n",
    "Next we will ask xarray to display some of the [parameters](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.html) of the `Dataset`. To do this simply return the contents of the `Dataset` variable name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Displaying `Dataset` properties\n",
    "> Try looking up the coordinates (coords), attributes (attrs) and data variables (data_vars) for our existing dataset.\n",
    "> Look at the output and think about what this tells us about our sample dataset.\n",
    "\n",
    "\n",
    "### Extracting `DataArrays` from a `Dataset`\n",
    "\n",
    "We have queried the dataset details about our `Datset` dimensions, coordinates and attributes. Next we will look at the variable data contained within the dataset. In the graphic above, there are two variables (temperature and precipitation). As described above, xarray stores these observations as a `DataArray`, which is similar to a conventional array you would find in numpy or matlab.\n",
    "\n",
    "Extracting a `DataArray` for processing is simple. From the `Dataset` metadata shown above, notice that the name of the climate variable is `'t2m'` (2 meter air temperature). Suppose we want to extract that array for processing and store it to a new variable called `temperature`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = ds['t2m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take a look at the contents of the `temperature` variable. Note that the associated coordinates and attributes get carried along for the ride. Also note that we are still not reading any data into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Label-based indexing\n",
    "\n",
    "---\n",
    "Questions:\n",
    "- \"How does the labeling of dimensions enhance the xarray workflow?\"\n",
    "objectives:\n",
    "- \"learning to locate data in xarray using integer- and label-based lookups\"\n",
    "- \"using named dimensions to located data\"\n",
    "keypoints:\n",
    "- \"xarray's labeled dimensions free the user from having to track positional ordering of dimensions when accessing data, creating a more simplified workflow\"\n",
    "---\n",
    "\n",
    "### Indexing\n",
    "\n",
    "Indexing is used to select specific elements from xarray files. Let's select some data from the 2-meter temperature `DataArray`. We know from the previous lesson that this `DataArray` has dimensions of time and two dimensional space (latitude and longitude).\n",
    "\n",
    "You are probably already used to conventional ways of indexing an array. You will know in advance that the first array index is time, the second is latitude, and so on. You would then use [positional indexing](http://xarray.pydata.org/en/stable/indexing.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['t2m'][0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of handling arrays should be familiar to anyone who has worked with arrays in MATLAB or NumPy. One challenge with this approach: it is not simple to associate an integer index position with something meaningful in our data. For example, we would have to write some function to map a specific date in the time dimension to its associated integer. Therefore, xarray lets us perform positional indexing using labels instead of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['t2m'].loc['1979-01-01T06:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but we still need to be keeping track of the fact that our index position 1 is the time dimension, position 2 is latitude, etc. So rather than looking up our dimension by position, xarray enables us to use the dimension name instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['t2m'].isel(time=0, latitude=0, longitude=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `isel` method refers to a selection by integer position. Finally, we can combine the benefits of both a labeled index and a named dimension as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['t2m'].sel(time='1979-01-01T06:00:00', latitude=75.0, longitude=180.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Slicing the data\n",
    "So far we have selected either a single array element, or all the elements along a dimension. Try using the __slice__ object to select some range of data along a dimension\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Plotting\n",
    "\n",
    "---\n",
    "Questions:\n",
    "- \"Does xarray have tools for visualizing the data?\"\n",
    "objectives:\n",
    "- \"learn simple methods to visualize subsets of xarray data in 1 or 2-dimensions\"\n",
    "keypoints:\n",
    "- \"xarray has plotting functinality that is a thin wrapper around the Matplotlib library\"\n",
    "- \"xarray uses syntax and function names from Matplotlib whenever possible\"\n",
    "---\n",
    "\n",
    "### Plotting data in 1 dimension\n",
    "\n",
    "Let's start visualizing some of the data slices we've been working on so far. We will begin by creating a new variable for plotting a 1-dimensional time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = ds['t2m'].sel(time=slice('1979-01-01T06:00:00', '1979-06-01T06:00:00'), latitude=75.0, longitude=180.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just like pandas, xarray has some very simple tools to enable quick visualizations of the data. Try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "time_series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your plots can be [customized](http://xarray.pydata.org/en/stable/plotting.html) using syntax that is very similar to Matplotlib. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.plot.line(color='green', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting data in 2 dimensions\n",
    "\n",
    "Since many xarray applications involve geospatial datasets, xarray's plotting extends to maps in 2 dimensions. Let's first select a 2-D subset of our data by choosing a single date and retaining all the latitude and longitude dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data = ds['t2m'].sel(time='1979-01-01T06:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above label-based lookup, we did not specify the latitude and longitude dimensions, in which case xarray assumes we want to return all elements in those dimensions.\n",
    "\n",
    "Now, similar to what we did for 1-D plots, simply call do the following to generate a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customization can occur following standard Matplotlib syntax. Note that before we use matplotlib, we will have to import that library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "map_data.plot(cmap=plt.cm.Blues)\n",
    "plt.title('ECMWF global air temperature data')\n",
    "plt.ylabel('latitude')\n",
    "plt.xlabel('longitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further customization can be added using Cartopy to add a projection, coastlines, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "ax = plt.axes(projection=ccrs.Orthographic(-80, 35))\n",
    "\n",
    "map_data.plot(ax=ax, cmap=plt.cm.Blues, transform=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "ax.coastlines()\n",
    "\n",
    "plt.title('ECMWF global air temperature data')\n",
    "plt.ylabel('latitude')\n",
    "plt.xlabel('longitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Arithmetic and aggregation\n",
    "\n",
    "---\n",
    "Questions:\n",
    "- \"How do I perform simple arithmetic operations on xarray objects?\"\n",
    "- \"How do I calculate statistics along a dimension of an xarray object?\"\n",
    "objectives:\n",
    "- \"using dimension names rather than integer axis numbers to perform common statistical arithmetic and aggregation functions\"\n",
    "keypoints:\n",
    "- \"xarray's labeled dimensions enable simplified arithmetic and data aggregation, enabling many powerful shortcuts\"\n",
    "---\n",
    "\n",
    "### Arithmetic\n",
    "\n",
    "Suppose we want to plot the difference in air temperature between January 1 in 1979 versus 1980. We can do this by taking advantage of xarray's labeled dimensions to simplify arithmetic operations on `DataArray` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature1 = ds['t2m'].sel(time='1979-01-01T06:00:00')\n",
    "temperature2 = ds['t2m'].sel(time='1980-01-01T06:00:00')\n",
    "delta = temperature1 - temperature2\n",
    "delta.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the subtraction is automatically vectorized over all array values, as in numpy.\n",
    "\n",
    "### Mathematical functions\n",
    "\n",
    "Now, sometimes we need to apply mathematical functions to array data in our analysis. A good example is wind data, which are often distributed as orthogonal \"u\" and \"v\" wind components. To calculate the wind magnitude we need to take the square root of the sum of the squares. For this we use numpy [ufunc](http://docs.scipy.org/doc/numpy/reference/ufuncs.html) commands that can operate on a `DataArray`. Let's look at our wind datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray.ufuncs as xu\n",
    "import matplotlib.pyplot as plt\n",
    "wind = open_tutorial_data('AK').sel(time=\"1984-01-01T06:00:00\")\n",
    "# wind = xr.open_mfdataset(r'c:/work/mnt/ecmwf/*wind_AK.nc')\n",
    "windspeed = xu.sqrt(wind.u10**2+wind.v10**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we introduced something new in opening our data. Since we need to access two netCDF files, we used xarray's [open_mfdataset](http://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html?highlight=open_mfdataset), which allows us to read any number of netCDF files into a single `Dataset`. Here we use a wildcard search to find the two wind datasets. Note that xarray exposes a wide range of mathematical functions this way, such as  `sin`, `cos`, etc.\n",
    "\n",
    "Now to plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed.plot.pcolormesh(cmap=plt.cm.Blues)\n",
    "plt.title('ECMWF wind speed and direction, June 1, 1984')\n",
    "plt.ylabel('latitude')\n",
    "plt.xlabel('longitude')\n",
    "x = windspeed.coords['longitude']\n",
    "y = windspeed.coords['latitude']\n",
    "plt.quiver(x, y, wind['u10'], wind['v10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "Aggregation methods can be applied to a `DataArray` over a specified dimension. Suppose we want to calculate the average June/July/August temperature for a particular year. Let's create a `DataArray` that slices out those months of data for a particular year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JJA = ds['t2m'].sel(time=slice('1979-06-01T06:00:00','1979-09-01T06:00:00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply apply the `mean` aggregation method over the time dimension and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JJA.mean(dim='time').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Aggregation\n",
    "Using the JJA DataArray created above, calculate the maximum air temperature during the JJA period at each latitude and longitude. Plot the result in degrees Celsius as a map. Also, calculate the standard deviation in global air temperature during the JJA period, and plot the results as a 1-D time series.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Groupby processing\n",
    "\n",
    "---\n",
    "Questions:\n",
    "- \"What is groupby processing and in what cases is it useful for scientific analysis of multidimensional arrays?\"\n",
    "objectives:\n",
    "- \"Learn the concepts of split/apply/combine and experimenting with xarray groupby processing\"\n",
    "keypoints:\n",
    "- \"xarray provides Pandas-like methods for performing data aggregation over defined groupings in the data\"\n",
    "---\n",
    "\n",
    "## GroupBy processing\n",
    "We often want to build a time series of change from spatially distributed data. For example, suppose we need to plot a time series of the global average air temperature across the entire period of our climate data record. To accomplish this, xarray has powerful [GroupBy](http://xarray.pydata.org/en/stable/groupby.html) processing tools, similar to the well known GROUP BY processing used in SQL. In all cases we **split** the data, **apply** a function to independent groups, and **combine** back into a known data structure.\n",
    "\n",
    "### Groupby processing: split\n",
    "\n",
    "We can `groupby` the name of a variable or coordinate. Either returns an xarray `groupby` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['t2m'].groupby('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby processing: apply\n",
    "Next we `apply` a function across the groupings set up in the xarray `groupby` process. When providing a single dimension to the `groupby` command, `apply` processes the function across the remaining dimensions. We could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    return x.mean()\n",
    "\n",
    "ds['t2m'].groupby('time').apply(mean).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, groupby objects have convenient shortcuts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['t2m'].groupby('time').mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the daily global average air temperature during the entire period of record.\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge: groupby\n",
    "Above we calculated daily global averages. Try to calculate the global _annual_ average instead, and plot the results as a 1-D time series.\n",
    "\n",
    "-----\n",
    "\n",
    "As a final example, here's a very interesting way to explore seasonal variations in temperature data using xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_by_season = ds['t2m'].groupby('time.season').mean('time')\n",
    "t2m_range = abs(ds_by_season.sel(season='JJA') - ds_by_season.sel(season='DJF'))\n",
    "t2m_range.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Out-of-core computation\n",
    "\n",
    "---\n",
    "Questions:\n",
    "- \"How can we do computations on array datasets that are too large to fit into memory on a local machine?\"\n",
    "objectives:\n",
    "- \"understand best practices for reading and storing large gridded datasets\"\n",
    "- \"using multi-threading libraries to facilitate manipulation of larger-than-memory grids\"\n",
    "keypoints:\n",
    "- dask integration with xarray allows you to work with large datasets that \"fit on disk\" rather than having to \"fit in memory\".\n",
    "- It is important to chunk the data correctly for this to work.\n",
    "---\n",
    "\n",
    "## Handling large grids\n",
    "\n",
    "When xarray carries out processing on an array it must load it into memory. Many datasets are becoming too large for this to be carried out on a typical laptop. For this reason, xarray integrates with a parallel computing library called [Dask](http://xray.readthedocs.org/en/stable/dask.html). Dask uses task scheduling and blocked algorithms to enable processing of datasets that \"fit on disk\" even if they do not \"fit in memory\".\n",
    "\n",
    "### Dask:\n",
    "* dask.array = numpy + threading\n",
    "* dask.bag = map, filter, toolz + multiprocessing\n",
    "* dask.dataframe = pandas + threading\n",
    "\n",
    "<br>\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/collections-schedulers.png\" width = \"500\" border = \"10\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src=\"http://mrocklin.github.com/blog/images/dask/embarrassing.gif\" width = \"500\" border = \"10\">\n",
    "<br>\n",
    "\n",
    "## Opening multiple netCDF files, and using Dask\n",
    "We will use the [mfdataset](http://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html#xarray.open_mfdataset) option that opens multiple files as a single xarray dataset. This automatically invokes the dask functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = open_tutorial_data('global', chunks={'time': 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk sizes:\n",
    "Without specifying chunk size, open_mfdataset chunks along existing dimensions. Getting the chunk size right is the crucial step to optimize working with xarray/dask. We recommend following [this advice](http://xarray.pydata.org/en/stable/dask.html?highlight=rechunk#chunking-and-performance). You should use chunk sizes of about 1 million elements. In our case: 480* 241 = 115680, so make the time chunk 10 to get around 1 million. Note that we are only chunking the time dimension. Choice depends on how you will be working with the data.\n",
    "\n",
    "Now when can carry out any processes on the `Dataset`, `dask` will be invoked. It is wise to include the `ProgressBar` tool from  `dask.diagnostics` to track the processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler\n",
    "from dask.diagnostics import visualize\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "import dask\n",
    "from multiprocessing.pool import ThreadPool\n",
    "dask.set_options(pool=ThreadPool(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ds['sst'].groupby('time.season').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dask.dot import dot_graph\n",
    "    dot_graph(seasons.data.dask)\n",
    "except:\n",
    "    print('no dot graph for you! :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    seasons = seasons.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize([prof, rprof, cprof])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Masking\n",
    "\n",
    "---\n",
    "Questions:\n",
    "- \"What is masking and how can it be used to analyze portions of a dataset\"\n",
    "objectives:\n",
    "- \"Learn the concepts of masking with xarray.\"\n",
    "keypoints:\n",
    "- \"xarray provides tools for creating and analyzing masked data.\"\n",
    "---\n",
    "\n",
    "## Masking with where:\n",
    "\n",
    "So far we have used indexing to return subsets of the original. The subset array shape will be different from the original. However, we often want to retain the array shape and mask out some observations. There are applications here in remote sensing, land cover modeling, etc.\n",
    "\n",
    "Suppose we need to determine which grid cells had temperatures > 20 deg C on June 21, 1984? We will use [where()](http://xarray.pydata.org/en/stable/indexing.html#masking-with-where) for this selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(time=\"1984-06-21\")['t2m'].where(ds.t2m > 293.15).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common Earth science application is to create land cover masks. Let's use the sea surface temperature field (sst) to build a land and ocean mask. We'll assign land a value of 1, and ocean a value of 2 (arbitrary). Note that the sst field currently has NaN for all land surfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sst.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buliding the mask:\n",
    "\n",
    "Here we'll use some lower-level numpy commands to build the mask (and we'll need to import the numpy library). The mask number depends on whether the cells are finite or NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = ds.sst.isel(time=0).isnull()\n",
    "\n",
    "mask_land = (nulls).astype(np.int)\n",
    "mask_ocean = 2 * (~nulls).astype(np.int)\n",
    "mask_array = mask_ocean + mask_land\n",
    "mask_array.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask as Coordinates\n",
    "We can keep the mask as a separate array entity, or, if we are using it routinely, there are advantages to [adding it](http://xarray.pydata.org/en/stable/data-structures.html#dataarray-coordinates) as a coordinate to the `DataArray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coords['mask'] = (('latitude', 'longitude'), mask_array)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the mask is integrated into the coordinates, we can easily apply the mask using `where()`. We can integrate this with statistical functions operating on the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "with ProgressBar():\n",
    "    ds['t2m'].mean('time').where(ds.mask == 1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Calculating a climate index\n",
    "Climate scientists commonly calculate mean diferences in sea and land surface temperatures. These differences are used as an index and correlated to other earth surface processes, such as ecological change. Using the air temperature dataset, calculate the mean annual difference in SST and t2m?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ready for more, head to [ndarrays_advanced](ndarrays_advanced.ipynb)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geohackenv",
   "language": "python",
   "name": "geohackenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
